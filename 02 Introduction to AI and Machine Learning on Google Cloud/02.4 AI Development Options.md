Basically, Google has options available for every experience level. I'm not sure what preconfigured models look like or how one would use them, but I've already used the other two types: we just coded out some basic logistic regressor and XGBoost models using SQL in BigQuery and I've built all sorts of ML models with Tensorflow/Keras in Colab.

![image](https://github.com/user-attachments/assets/dfb9603c-50be-47d1-be5f-c66e2b94636a)

Drawbacks of each:

Pre-trained APis, BigQuery, AutoML: [Only certain types of models are available](https://cloud.google.com/bigquery/docs/bqml-introduction).

The way they ordered things in the slide below is weird. BigQuery requires SQL coding, and AutoML doesn't - you build models with a point and click GUI. Whatever.

![image](https://github.com/user-attachments/assets/0e6b6986-2cb4-4b6e-a9c0-a324ee0afe06)

### Useful comparison between products

This sums it up. That said, they say you can't create ML audio models with a DIY approach, but I'm pretty sure that's not true. Someone in my bootcamp used Tensorflow [to create an ML model with audio](https://github.com/benjmcpeek/Audio-Classification-Model-Major-Minor/blob/main/code/03_final_models.ipynb). Many of my classmates used pre-trained models to supplement their models, which is a bennefit of the API approach: you don't have to spend time training. It's just not that flexible and you're stuck in the Google ecosystem.

![image](https://github.com/user-attachments/assets/3e721d27-bc6c-4cb5-ad91-b843cb9998e9)

### AutoML use case

Ok, I understand the use-case for AutoML now: you can create custom models without coding at all, whereas with BigQuery you're more limited, I guess? I want to understand what they mean by "custom model" a bit deeper, since it seems like we were able to pass in quite a few params into our XGBoost model like regularization etc. Also, how many people would be able to successfully tune the hyperparameters of an advanced ML model but *don't* know how to code Python? Is really that much faster to do it no-code? Seems like a strange use-case, but maybe it'll make more sense as time goes on.

![image](https://github.com/user-attachments/assets/f951c3b4-484b-4e3a-b306-c7214bad0cac)


# A deeper breakdown of each type
## API

As the lecture points out, a major challenge for building a custom ML model is not having enough data. A way to get around this challenge is to use a pre-trained model. If you are attempting to classify handwriting or something, there's really no need to make a custom image analysis model from scratch as this is one of the most common ML tasks out there - you might as well use a premade model. This applies to other use-cases, like text-to-speech (and vice-versa), etc.

Paraphrasing: "If you just want to charge your phone, all you care about is which adapter to use, not what's going on behind the wall or how to build an electric network", i.e. APIs allow you to get the job done as quickly as possible (if an API exists for your job and you're willing to pay the premium). 

As a sidebar for my own edification, this is an example of a **Product as a Service (PaaS)**, i.e. the most abstracted it really gets from simply renting server time.

![image](https://github.com/user-attachments/assets/d4603e7a-0789-4b73-97e7-2be635af506b)

### So, what *can* GCP APIs do?

![image](https://github.com/user-attachments/assets/4328be5d-545e-437c-9af3-aed8c772ebd8)

It should be noted that you can use these APIs, or similar pre-trained models within a custom ML approach, and they aren't unique to GCP. In fact, many are totally free to use. For instance, you can use the Natural Language Toolkit (NLTK) has a tool called Vader which analyzes the sentiment of tokenized text, whether that's entire sentences or individual words. It's able to do that because it's a pre-trained ML model, and you can conveniently use it in your own custom models, without needing to train a sentiment analyzer from scratch. 

![vader-sentiment-analysis-1-1-715528563](https://github.com/user-attachments/assets/99ce8cd3-6850-48ae-8456-b18863ab4f36) [Image source](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Falgotrading101.com%2Flearn%2Fwp-content%2Fuploads%2F2019%2F12%2Fvader-sentiment-analysis-1-1.png&f=1&nofb=1&ipt=3940ecb79be4267a3c642812fd506017b386583c7b8b429704a5bafba0cfd48d&ipo=images)

nltk.sentiment.vader is able to analyze sentiment - the options available for GCP text analyzis are more advanced, although I would assume a similar free-to-use option exists for entity analysis etc:

![image](https://github.com/user-attachments/assets/d516f44d-a42b-4e92-b874-7836ffda1610)

Clearly, this would be pretty useful for classifying posts based on keywords and generating brief summaries. In general, extracting information from text (unstructured data) which can be put into a table (structured) is pretty crucial, since the former not only takes up far more space, but can be inconvenient to work with directly.

### Generative AI APIs

This could be the most useful API type in my opinion, since it's highly unlikely that you would code up a complete chatbot from sratch. These APIs can generate text (conversational, etc.), write code, or create images. Seems pretty straightforward.

![image](https://github.com/user-attachments/assets/ab35686c-e2e8-428f-bc2e-a8e7c4a8a6a9)

## Vertex AI

This is the next level down in terms of abstraction: you get more control over the structure of the model, but each phase (data storage, cleaning, design, deployment and managment) is contained with GCP, which can help with challenges you'd normally face, like storing large amounts of data, etc.

![image](https://github.com/user-attachments/assets/1a0399e3-1723-4af3-ae62-1b41e93dd1ab)

Notably, this doesn't actually involve any coding, although users get large amounts of control over model type and hyperparameter tuning. As I noted before, this seems odd since I feel like the main obsticle to being a good ML engineer probably isn't coding ability, but more *actually* understanding how the model works and what the hyperparameters do. But I guess, you could always just get in there and mess around with the options until you create something halfway decent, it just might waste a lot of time and resources. Maybe this will make more sense when I get to try Vertex. At the moment though, I don't see why anyone would want to use it over a custom ML model, unless they got hired as a data scientist despite not knowing how to code.

![image](https://github.com/user-attachments/assets/974a4f6d-b6e9-4919-8f05-758683a6663a)

Ok never mind, you actually can code up your own custom models. AutoML is the tool for no-code solutions. I can see why having a unified environment to house your entire pipeline (storage, development, deployment, operations/maintnance) would be useful.

![image](https://github.com/user-attachments/assets/cb610310-89ff-4f97-bb9f-25e90ee82b10)

## AutoML

There are two main ideas here: transfer learning and neural architecture search

![image](https://github.com/user-attachments/assets/dd40f9dd-a195-4610-a437-fdc3eed4c986)

* Transfer learning: if you have a relatively small amount of data, you can use models trained on similar data to enhance performance. For example, if you're trying to figure out which images contain cars, you can use transfer learning from a model trained to classify objects in general e.g. [ImageNet](https://image-net.org/).

* Neural architecture search: automatically attempts a variety of model types and hyperparameters.

As noted in Phase 3, AutoML uses bootstrap aggregating (bagging) to combine the best atributes of ~10 models, which might be done by averaging the predictions of the top `n` models (lessening overfitting).

I can sort-of see the value with this approach now. In many cases, it would be time-consuming (although not impossible) for a human to replicate this sort of approach. It basically just makes high-quality results faster while allowing lower-skilled people to perform the job.

## Custom training

### TensorFlow

I don't have any comments on the next slide - this is something I need to dig deeper into. 

![image](https://github.com/user-attachments/assets/66b4a9ba-a226-4421-85a0-3af69688887a)

**Steps for building a TF Keras model**
1. Create the model by defining the layers you'll use.
![image](https://github.com/user-attachments/assets/ff5dbef7-888a-4443-8dac-456566c7936b)

Code: 
![image](https://github.com/user-attachments/assets/fd5caa7a-780f-4aa2-b989-45f6e52d996d)

How layers are used in different types of neutral networks:
![image](https://github.com/user-attachments/assets/4b495911-216c-44b6-9310-440ff4c5a73b)
[Image source](https://miro.medium.com/v2/resize:fit:2000/1*cuTSPlTq0a_327iTPJyD-Q.png). Some of these are pretty obscure/not commonly used.


2. 



